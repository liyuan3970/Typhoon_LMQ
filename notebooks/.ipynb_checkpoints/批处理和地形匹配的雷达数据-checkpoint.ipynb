{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'f1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-1c87852acc76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0mdataio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-1c87852acc76>\u001b[0m in \u001b[0;36mdataio\u001b[0;34m(filename, file)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdataio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mfipath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/home/liyuan3970/Data/data/meto_data/radar_typhoon_liqima/wenzhou_rada/data_120*181/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mbasedata1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mradar_io\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0mr1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbasedata1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToPyartRadar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mcp1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'reflectivity'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'f1' is not defined"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "import multidop\n",
    "import pyart\n",
    "import tempfile\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import pyart\n",
    "import pydda\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from pycwr.io.auto_io import radar_io \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import pyart\n",
    "\n",
    "path ='/home/liyuan3970/Data/data/meto_data/radar_typhoon_liqima/wenzhou_rada/after_land'\n",
    "file = os.listdir(path)\n",
    "file.sort()\n",
    "# for i in range(len(file)):\n",
    "#     file[i] = path +file[i]\n",
    "#print(file)\n",
    "\n",
    "filename = r\"/home/liyuan3970/study_demo/met_plot/利齐马/src/Z_RADR_I_Z9577_20190809162900_O_DOR_SA_CAP.bin.bz2\"\n",
    "\n",
    "def dataio(filename,file):\n",
    "    fipath = '/home/liyuan3970/Data/data/meto_data/radar_typhoon_liqima/wenzhou_rada/data_120*181/'\n",
    "    basedata1 = radar_io(filename) \n",
    "    r1 = basedata1.ToPyartRadar()\n",
    "    cp1 = deepcopy(r1.fields['reflectivity']['data'])\n",
    "    r1.add_field_like('reflectivity', 'DT', cp1, replace_existing=True)\n",
    "    gatefilter1 = pyart.correct.GateFilter(r1)\n",
    "    gatefilter1.exclude_masked('reflectivity')\n",
    "    corr_vel1 = pyart.correct.dealias_region_based(\n",
    "        r1, vel_field='velocity', keep_original=False, \n",
    "        gatefilter=gatefilter1, centered=True)\n",
    "    r1.add_field('VT', corr_vel1, replace_existing=True)\n",
    "    # The analysis engine currently expects the \"missing_value\" attribute\n",
    "    r1.fields['DT']['missing_value'] = 1.0 * r1.fields['DT']['_FillValue']\n",
    "    r1.fields['VT']['missing_value'] = 1.0 * r1.fields['VT']['_FillValue']\n",
    "    radars1 =[r1]\n",
    "    grids1 = pyart.map.grid_from_radars(\n",
    "             radars1, grid_shape=(23, 120, 181),\n",
    "            grid_limits=((0, 15000.0),(-150000, 150000), (-150000, 150000)),\n",
    "            fields=['reflectivity','DT','VT'], gridding_algo=\"map_gates_to_grid\",\n",
    "            weighting_function='BARNES')\n",
    "    # Set initialization and do retrieval\n",
    "    u_init, v_init, w_init = make_initialization_from_era_interim(grids1, vel_field='VT')\n",
    "    new_grids = pydda.retrieval.get_dd_wind_field([grids1],\n",
    "                                                  u_init, v_init, w_init,\n",
    "                                                  vel_name='VT', refl_field='DT',\n",
    "                                                  mask_outside_opt=True)\n",
    "    pyart.io.write_grid(fipath+file+'.nc', new_grids[0])\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "for fi in file:\n",
    "    filename = path+fi\n",
    "    dataio(filename,fi)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## You are using the Python ARM Radar Toolkit (Py-ART), an open source\n",
      "## library for working with weather radar data. Py-ART is partly\n",
      "## supported by the U.S. Department of Energy as part of the Atmospheric\n",
      "## Radiation Measurement (ARM) Climate Research Facility, an Office of\n",
      "## Science user facility.\n",
      "##\n",
      "## If you use this software to prepare a publication, please cite:\n",
      "##\n",
      "##     JJ Helmus and SM Collis, JORS 2016, doi: 10.5334/jors.119\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liyuan3970/anaconda3/lib/python3.7/site-packages/yaml/constructor.py:126: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  if not isinstance(key, collections.Hashable):\n",
      "/home/liyuan3970/anaconda3/lib/python3.7/site-packages/h5py/_hl/base.py:19: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import (Mapping, MutableMapping, KeysView,\n",
      "/home/liyuan3970/anaconda3/lib/python3.7/site-packages/h5py/_hl/base.py:19: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import (Mapping, MutableMapping, KeysView,\n",
      "/home/liyuan3970/anaconda3/lib/python3.7/site-packages/h5py/_hl/base.py:19: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import (Mapping, MutableMapping, KeysView,\n",
      "/home/liyuan3970/anaconda3/lib/python3.7/site-packages/h5py/_hl/base.py:19: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import (Mapping, MutableMapping, KeysView,\n",
      "/home/liyuan3970/anaconda3/lib/python3.7/site-packages/h5py/_hl/base.py:19: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import (Mapping, MutableMapping, KeysView,\n",
      "/home/liyuan3970/anaconda3/lib/python3.7/site-packages/_pytest/mark/structures.py:443: DeprecationWarning: The usage of `cmp` is deprecated and will be removed on or after 2021-06-01.  Please use `eq` and `order` instead.\n",
      "  @attr.s(cmp=False, hash=False)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pyart\n",
    "import gc\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "# We want cfgrib to be an optional dependency to ensure Windows compatibility\n",
    "try:\n",
    "    import cfgrib\n",
    "    CFGRIB_AVAILABLE = True\n",
    "except:\n",
    "    CFGRIB_AVAILABLE = False\n",
    "\n",
    "# We really only need the API to download the data, make ECMWF API an\n",
    "# optional dependency since not everyone will have a login from the start.\n",
    "try:\n",
    "    from ecmwfapi import ECMWFDataServer\n",
    "    ECMWF_AVAILABLE = True\n",
    "except:\n",
    "    ECMWF_AVAILABLE = False\n",
    "\n",
    "from netCDF4 import Dataset\n",
    "from datetime import datetime, timedelta\n",
    "from scipy.interpolate import RegularGridInterpolator, interp1d, griddata\n",
    "from scipy.interpolate import NearestNDInterpolator\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "\n",
    "def make_initialization_from_era_interim(Grid, file_name=None, vel_field=None,\n",
    "                                         dest_era_file=None):\n",
    "    \"\"\"\n",
    "    This function will read ERA Interim in NetCDF format and add it\n",
    "    to the Py-ART grid specified by Grid. PyDDA will automatically download\n",
    "    the ERA Interim data that you need for the scan. It will chose the domain\n",
    "    that is enclosed by the analysis grid and the time period that is closest\n",
    "    to the scan. It will then do a Nearest Neighbor interpolation of the\n",
    "    ERA-Interim u and v winds to the analysis grid.\n",
    "\n",
    "    You need to have the ECMWF API and an ECMWF account set up in order to\n",
    "    use this feature. Go to this website for instructions on installing the\n",
    "    API and setting up your account:\n",
    "\n",
    "    https://confluence.ecmwf.int/display/WEBAPI/Access+ECMWF+Public+Datasets\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Grid: Py-ART Grid\n",
    "        The input Py-ART Grid to modify.\n",
    "    file_name: str or None\n",
    "        The netCDF file containing the ERA Interim data. If the web\n",
    "        API is experiencing delays, it is better to use it to download the\n",
    "        file and then refer to it here. If this file does not exist\n",
    "        PyDDA will use the API to create the file.\n",
    "    vel_field: str or None\n",
    "        The name of the velocity field in the Py-ART grid. Set to None to\n",
    "        have Py-DDA attempt to automatically detect it.\n",
    "    dest_era_file:\n",
    "        If this is not None, PyDDA will save the interpolated grid into this file.\n",
    "    Returns\n",
    "    -------\n",
    "    new_Grid: Py-ART Grid\n",
    "        The Py-ART Grid with the ERA Interim data added into the \"u_erainterim\",\n",
    "        \"v_erainterim\", and \"w_erainterim\" fields.\n",
    "\n",
    "    \"\"\"\n",
    "    if vel_field is None:\n",
    "        vel_field = pyart.config.get_field_name('corrected_velocity')\n",
    "\n",
    "    if ECMWF_AVAILABLE is False and file_name is None:\n",
    "        raise (ModuleNotFoundError,\n",
    "               (\"The ECMWF API is not installed. Go to\" +\n",
    "                \"https://confluence.ecmwf.int/display/WEBAPI\" +\n",
    "                \"/Access+ECMWF+Public+Datasets\" +\n",
    "                \" in order to use the auto download feature.\"))\n",
    "\n",
    "    grid_time = datetime.strptime(Grid.time[\"units\"],\n",
    "                                  \"seconds since %Y-%m-%dT%H:%M:%SZ\")\n",
    "    hour_rounded_to_nearest_3 = int(3 * round(float(grid_time.hour)/3))\n",
    "\n",
    "    if hour_rounded_to_nearest_3 == 24:\n",
    "        grid_time = grid_time + timedelta(days=1)\n",
    "        grid_time = datetime(grid_time.year, grid_time.month,\n",
    "                             grid_time.day, 0, grid_time.minute,\n",
    "                             grid_time.second)\n",
    "    else:\n",
    "        grid_time = datetime(grid_time.year, grid_time.month,\n",
    "                             grid_time.day,\n",
    "                             hour_rounded_to_nearest_3,\n",
    "                             grid_time.minute, grid_time.second)\n",
    "\n",
    "    if file_name is not None:\n",
    "        if not os.path.isfile(file_name):\n",
    "            raise FileNotFoundError(file_name + \" not found!\")\n",
    "\n",
    "    if file_name is None:\n",
    "        print(\"Download ERA Interim data...\")\n",
    "        # ERA interim data is in pressure coordinates\n",
    "        # Retrieve u, v, w, and geopotential\n",
    "        # Geopotential is needed to convert into height coordinates\n",
    "\n",
    "        retrieve_dict = {}\n",
    "        retrieve_dict['stream'] = \"oper\"\n",
    "        retrieve_dict['levtype'] = \"pl\"\n",
    "        retrieve_dict['param'] = \"131.128/132.128/135.128/129.128\"\n",
    "        retrieve_dict['dataset'] = \"interim\"\n",
    "        retrieve_dict['levelist'] = (\"1/2/3/5/7/10/20/30/50/70/100/125/150/\" +\n",
    "                                     \"175/200/225/250/300/350/400/450/500/\" +\n",
    "                                     \"550/600/650/700/750/775/800/825/850/\" +\n",
    "                                     \"875/900/925/950/975/1000\")\n",
    "        retrieve_dict['step'] = \"%d\" % grid_time.hour\n",
    "        retrieve_dict['date'] = grid_time.strftime(\"%Y-%m-%d\")\n",
    "        retrieve_dict['class'] = \"ei\"\n",
    "        retrieve_dict['grid'] = \"0.75/0.75\"\n",
    "        N = \"%4.1f\" % Grid.point_latitude[\"data\"].max()\n",
    "        S = \"%4.1f\" % Grid.point_latitude[\"data\"].min()\n",
    "        E = \"%4.1f\" % Grid.point_longitude[\"data\"].max()\n",
    "        W = \"%4.1f\" % Grid.point_longitude[\"data\"].min()\n",
    "\n",
    "        retrieve_dict['area'] = N + \"/\" + W + \"/\" + S + \"/\" + E\n",
    "        retrieve_dict['format'] = \"netcdf\"\n",
    "        if dest_era_file is not None:\n",
    "            retrieve_dict['target'] = dest_era_file\n",
    "            file_name = dest_era_file\n",
    "        else:\n",
    "            tfile = tempfile.NamedTemporaryFile()\n",
    "            retrieve_dict['target'] = tfile.name\n",
    "            file_name = tfile.name\n",
    "        server = ECMWFDataServer()\n",
    "        server.retrieve(retrieve_dict)\n",
    "\n",
    "    ERA_grid = Dataset(file_name, mode='r')\n",
    "    base_time = datetime.strptime(ERA_grid.variables[\"time\"].units,\n",
    "                                  \"hours since %Y-%m-%d %H:%M:%S.%f\")\n",
    "    time_seconds = ERA_grid.variables[\"time\"][:]\n",
    "    our_time = np.array([base_time + timedelta(seconds=int(x)) for x in time_seconds])\n",
    "    time_step = np.argmin(np.abs(base_time - grid_time))\n",
    "\n",
    "    analysis_grid_shape = Grid.fields[vel_field]['data'].shape\n",
    "\n",
    "    height_ERA = ERA_grid.variables[\"z\"][:]\n",
    "    u_ERA = ERA_grid.variables[\"u\"][:]\n",
    "    v_ERA = ERA_grid.variables[\"v\"][:]\n",
    "    w_ERA = ERA_grid.variables[\"w\"][:]\n",
    "    lon_ERA = ERA_grid.variables[\"longitude\"][:]\n",
    "    lat_ERA = ERA_grid.variables[\"latitude\"][:]\n",
    "    radar_grid_lat = Grid.point_latitude['data']\n",
    "    radar_grid_lon = Grid.point_longitude['data']\n",
    "    radar_grid_alt = Grid.point_z['data']\n",
    "    u_flattened = u_ERA[time_step].flatten()\n",
    "    v_flattened = v_ERA[time_step].flatten()\n",
    "    w_flattened = w_ERA[time_step].flatten()\n",
    "\n",
    "    the_shape = u_ERA.shape\n",
    "    lon_mgrid, lat_mgrid = np.meshgrid(lon_ERA, lat_ERA)\n",
    "\n",
    "    lon_mgrid = np.tile(lon_mgrid, (the_shape[1], 1, 1))\n",
    "    lat_mgrid = np.tile(lat_mgrid, (the_shape[1], 1, 1))\n",
    "    lon_flattened = lon_mgrid.flatten()\n",
    "    lat_flattened = lat_mgrid.flatten()\n",
    "    height_flattened = height_ERA[time_step].flatten()\n",
    "    height_flattened -= Grid.radar_altitude[\"data\"]\n",
    "\n",
    "    u_interp = NearestNDInterpolator(\n",
    "        (height_flattened, lat_flattened, lon_flattened),\n",
    "        u_flattened, rescale=True)\n",
    "    v_interp = NearestNDInterpolator(\n",
    "        (height_flattened, lat_flattened, lon_flattened),\n",
    "        v_flattened, rescale=True)\n",
    "    w_interp = NearestNDInterpolator(\n",
    "        (height_flattened, lat_flattened, lon_flattened),\n",
    "        w_flattened, rescale=True)\n",
    "    u_new = u_interp(radar_grid_alt, radar_grid_lat, radar_grid_lon)\n",
    "    v_new = v_interp(radar_grid_alt, radar_grid_lat, radar_grid_lon)\n",
    "    w_new = w_interp(radar_grid_alt, radar_grid_lat, radar_grid_lon)\n",
    "\n",
    "    # Free up memory\n",
    "    ERA_grid.close()\n",
    "\n",
    "    if 'tfile' in locals():\n",
    "        tfile.close()\n",
    "\n",
    "    return u_new, v_new, w_new\n",
    "\n",
    "\n",
    "def make_constant_wind_field(Grid, wind=(0.0, 0.0, 0.0), vel_field=None):\n",
    "    \"\"\"\n",
    "    This function makes a constant wind field given a wind vector.\n",
    "\n",
    "    This function is useful for specifying the intialization arrays\n",
    "    for get_dd_wind_field.\n",
    "\n",
    "    Parameters\n",
    "    ==========\n",
    "\n",
    "    Grid: Py-ART Grid object\n",
    "        This is the Py-ART Grid containing the coordinates for the analysis\n",
    "        grid.\n",
    "    wind: 3-tuple of floats\n",
    "        The 3-tuple specifying the (u,v,w) of the wind field.\n",
    "    vel_field: String\n",
    "        The name of the velocity field. None will automatically\n",
    "        try to detect this field.\n",
    "\n",
    "    Returns\n",
    "    =======\n",
    "\n",
    "    u: 3D float array\n",
    "        Returns a 3D float array containing the u component of the wind field.\n",
    "        The shape will be the same shape as the fields in Grid.\n",
    "    v: 3D float array\n",
    "        Returns a 3D float array containing the v component of the wind field.\n",
    "        The shape will be the same shape as the fields in Grid.\n",
    "    w: 3D float array\n",
    "        Returns a 3D float array containing the u component of the wind field.\n",
    "        The shape will be the same shape as the fields in Grid.\n",
    "    \"\"\"\n",
    "\n",
    "    # Parse names of velocity field\n",
    "    if vel_field is None:\n",
    "        vel_field = pyart.config.get_field_name('corrected_velocity')\n",
    "    analysis_grid_shape = Grid.fields[vel_field]['data'].shape\n",
    "\n",
    "    u = wind[0]*np.ones(analysis_grid_shape)\n",
    "    v = wind[1]*np.ones(analysis_grid_shape)\n",
    "    w = wind[2]*np.ones(analysis_grid_shape)\n",
    "    u = np.ma.filled(u, 0)\n",
    "    v = np.ma.filled(v, 0)\n",
    "    w = np.ma.filled(w, 0)\n",
    "    return u, v, w\n",
    "\n",
    "\n",
    "def make_wind_field_from_profile(Grid, profile, vel_field=None):\n",
    "    \"\"\"\n",
    "    This function makes a 3D wind field from a sounding.\n",
    "\n",
    "    This function is useful for using sounding data as an initialization\n",
    "    for get_dd_wind_field.\n",
    "\n",
    "    Parameters\n",
    "    ==========\n",
    "    Grid: Py-ART Grid object\n",
    "        This is the Py-ART Grid containing the coordinates for the analysis\n",
    "        grid.\n",
    "    profile: PyART HorizontalWindProfile\n",
    "        This is the HorizontalWindProfile of the sounding\n",
    "    wind: 3-tuple of floats\n",
    "        The 3-tuple specifying the (u,v,w) of the wind field.\n",
    "    vel_field: String\n",
    "        The name of the velocity field in Grid. None will automatically\n",
    "        try to detect this field.\n",
    "\n",
    "    Returns\n",
    "    =======\n",
    "\n",
    "    u: 3D float array\n",
    "        Returns a 3D float array containing the u component of the wind field.\n",
    "        The shape will be the same shape as the fields in Grid.\n",
    "    v: 3D float array\n",
    "        Returns a 3D float array containing the v component of the wind field.\n",
    "        The shape will be the same shape as the fields in Grid.\n",
    "    w: 3D float array\n",
    "        Returns a 3D float array containing the u component of the wind field.\n",
    "        The shape will be the same shape as the fields in Grid.\n",
    "        \"\"\"\n",
    "    # Parse names of velocity field\n",
    "    if vel_field is None:\n",
    "        vel_field = pyart.config.get_field_name('corrected_velocity')\n",
    "    analysis_grid_shape = Grid.fields[vel_field]['data'].shape\n",
    "    u = np.ones(analysis_grid_shape)\n",
    "    v = np.ones(analysis_grid_shape)\n",
    "    w = np.zeros(analysis_grid_shape)\n",
    "    u_back = profile.u_wind\n",
    "    v_back = profile.v_wind\n",
    "    z_back = profile.height\n",
    "    u_interp = interp1d(\n",
    "        z_back, u_back, bounds_error=False, fill_value='extrapolate')\n",
    "    v_interp = interp1d(\n",
    "        z_back, v_back, bounds_error=False, fill_value='extrapolate')\n",
    "    u_back2 = u_interp(np.asarray(Grid.z['data']))\n",
    "    v_back2 = v_interp(np.asarray(Grid.z['data']))\n",
    "    for i in range(analysis_grid_shape[0]):\n",
    "        u[i] = u_back2[i]\n",
    "        v[i] = v_back2[i]\n",
    "    u = np.ma.filled(u, 0)\n",
    "    v = np.ma.filled(v, 0)\n",
    "    w = np.ma.filled(w, 0)\n",
    "    return u, v, w\n",
    "\n",
    "\n",
    "def make_background_from_wrf(Grid, file_path, wrf_time,\n",
    "                             radar_loc, vel_field=None):\n",
    "    \"\"\"\n",
    "    This function makes an initalization field based off of the u and w\n",
    "    from a WRF run. Only u and v are used from the WRF file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Grid: Py-ART Grid object\n",
    "        This is the Py-ART Grid containing the coordinates for the\n",
    "        analysis grid.\n",
    "    file_path: str\n",
    "        This is the path to the WRF grid\n",
    "    wrf_time: datetime\n",
    "        The timestep to derive the intialization field from.\n",
    "    radar_loc: tuple\n",
    "        The (X, Y) location of the radar in the WRF grid. The output\n",
    "        coordinate system will be centered around this location\n",
    "        and given the same grid specification that is specified\n",
    "        in Grid.\n",
    "    vel_field: str, or None\n",
    "        This string contains the name of the velocity field in the\n",
    "        Grid. None will try to automatically detect this value.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    u: 3D ndarray\n",
    "        The initialization u field.\n",
    "        The shape will be the same shape as the fields in Grid and will\n",
    "        correspond to the same x, y, and z locations as in Grid.\n",
    "    v: 3D ndarray\n",
    "        The initialization v field.\n",
    "        The shape will be the same shape as the fields in Grid and will\n",
    "        correspond to the same x, y, and z locations as in Grid.\n",
    "    w: 3D ndarray\n",
    "        The initialization w field. The shape will be the same shape \n",
    "        as the fields in Grid and will correspond to the same x, y, and z\n",
    "        locations as in Grid.\n",
    "\n",
    "    \"\"\"\n",
    "    # Parse names of velocity field\n",
    "    if vel_field is None:\n",
    "        vel_field = pyart.config.get_field_name('corrected_velocity')\n",
    "\n",
    "    analysis_grid_shape = Grid.fields[vel_field]['data'].shape\n",
    "    u = np.ones(analysis_grid_shape)\n",
    "    v = np.ones(analysis_grid_shape)\n",
    "    w = np.zeros(analysis_grid_shape)\n",
    "\n",
    "    # Load WRF grid\n",
    "    wrf_cdf = Dataset(file_path, mode='r')\n",
    "    W_wrf = wrf_cdf.variables['W'][:]\n",
    "    V_wrf = wrf_cdf.variables['V'][:]\n",
    "    U_wrf = wrf_cdf.variables['U'][:]\n",
    "    PH_wrf = wrf_cdf.variables['PH'][:]\n",
    "    PHB_wrf = wrf_cdf.variables['PHB'][:]\n",
    "    alt_wrf = (PH_wrf+PHB_wrf)/9.81\n",
    "\n",
    "    new_grid_x = Grid.point_x['data']\n",
    "    new_grid_y = Grid.point_y['data']\n",
    "    new_grid_z = Grid.point_z['data']\n",
    "\n",
    "    # Find timestep from datetime\n",
    "    time_wrf = wrf_cdf.variables['Times']\n",
    "    ntimes = time_wrf.shape[0]\n",
    "    dts_wrf = []\n",
    "    for i in range(ntimes):\n",
    "        x = ''.join([x.decode() for x in time_wrf[i]])\n",
    "        dts_wrf.append(datetime.strptime(x, '%Y-%m-%d_%H:%M:%S'))\n",
    "\n",
    "    dts_wrf = np.array(dts_wrf)\n",
    "    timestep = np.where(dts_wrf == wrf_time)\n",
    "    if(len(timestep[0]) == 0):\n",
    "        raise ValueError((\"Time \" + str(wrf_time) + \" not found in WRF file!\"))\n",
    "\n",
    "    x_len = wrf_cdf.__getattribute__('WEST-EAST_GRID_DIMENSION')\n",
    "    y_len = wrf_cdf.__getattribute__('SOUTH-NORTH_GRID_DIMENSION')\n",
    "    dx = wrf_cdf.DX\n",
    "    dy = wrf_cdf.DY\n",
    "    x = np.arange(0, x_len)*dx-radar_loc[0]*1e3\n",
    "    y = np.arange(0, y_len)*dy-radar_loc[1]*1e3\n",
    "    z = np.mean(alt_wrf[timestep[0], :, :, :], axis=(0, 2, 3))\n",
    "    x, y, z = np.meshgrid(x, y, z)\n",
    "    z = np.squeeze(alt_wrf[timestep[0], :, :, :])\n",
    "\n",
    "    z_stag = (z[1:, :, :]+z[:-1, :, :])/2.0\n",
    "    x_stag = (x[:, :, 1:]+x[:, :, :-1])/2.0\n",
    "    y_stag = (y[:, 1:, :]+y[:, :-1, :])/2.0\n",
    "\n",
    "    W_wrf = np.squeeze(W_wrf[timestep[0], :, :, :])\n",
    "    V_wrf = np.squeeze(V_wrf[timestep[0], :, :, :])\n",
    "    U_wrf = np.squeeze(U_wrf[timestep[0], :, :, :])\n",
    "\n",
    "    w = griddata((z_stag, y, x), W_wrf,\n",
    "                 (new_grid_z, new_grid_y, new_grid_x), fill_value=0.)\n",
    "    v = griddata((z, y_stag, x), V_wrf,\n",
    "                 (new_grid_z, new_grid_y, new_grid_x), fill_value=0.)\n",
    "    u = griddata((z, y, x_stag), U_wrf,\n",
    "                 (new_grid_z, new_grid_y, new_grid_x), fill_value=0.)\n",
    "\n",
    "    return u, v, w\n",
    "\n",
    "\n",
    "def make_intialization_from_hrrr(Grid, file_path):\n",
    "    \"\"\"\n",
    "    This function will read an HRRR GRIB2 file and return initial guess\n",
    "    u, v, and w fields from the model\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Grid: Py-ART Grid\n",
    "        The Py-ART Grid to use as the grid specification. The HRRR values\n",
    "    will be interpolated to the Grid's specficiation and added as a field.\n",
    "    file_path: string\n",
    "        The path to the GRIB2 file to load.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Grid: Py-ART Grid\n",
    "        This returns the Py-ART grid with the HRRR u, and v fields added.\n",
    "        The shape will be the same shape as the fields in Grid and will\n",
    "        correspond to the same x, y, and z locations as in Grid.\n",
    "    \"\"\"\n",
    "\n",
    "    if(CFGRIB_AVAILABLE is False):\n",
    "        raise RuntimeError((\"The cfgrib optional dependency needs to be \" +\n",
    "                            \"installed for the HRRR integration feature.\"))\n",
    "\n",
    "    the_grib = cfgrib.open_file(\n",
    "        file_path, filter_by_keys={'typeOfLevel': 'isobaricInhPa'})\n",
    "\n",
    "    # Load the HRR data and tranform longitude coordinates\n",
    "    grb_u = the_grib.variables['u']\n",
    "    grb_v = the_grib.variables['v']\n",
    "    grb_w = the_grib.variables['w']\n",
    "    gh = the_grib.variables['gh']\n",
    "\n",
    "    lat = the_grib.variables['latitude'].data[:, :]\n",
    "    lon = the_grib.variables['longitude'].data[:, :]\n",
    "    lon[lon > 180] = lon[lon > 180] - 360\n",
    "\n",
    "    # Convert geometric height to geopotential height\n",
    "    EARTH_MEAN_RADIUS = 6.3781e6\n",
    "    gh = gh.data[:, :, :]\n",
    "    height = (EARTH_MEAN_RADIUS*gh)/(EARTH_MEAN_RADIUS-gh)\n",
    "    height = height - Grid.radar_altitude['data']\n",
    "\n",
    "    radar_grid_lat = Grid.point_latitude['data']\n",
    "    radar_grid_lon = Grid.point_longitude['data']\n",
    "    radar_grid_alt = Grid.point_z['data']\n",
    "    lat_min = radar_grid_lat.min()\n",
    "    lat_max = radar_grid_lat.max()\n",
    "    lon_min = radar_grid_lon.min()\n",
    "    lon_max = radar_grid_lon.max()\n",
    "    lon_r = np.tile(lon, (height.shape[0], 1, 1))\n",
    "    lat_r = np.tile(lat, (height.shape[0], 1, 1))\n",
    "    lon_flattened = lon_r.flatten()\n",
    "    lat_flattened = lat_r.flatten()\n",
    "    height_flattened = gh.flatten()\n",
    "    the_box = np.where(np.logical_and.reduce(\n",
    "                       (lon_flattened >= lon_min,\n",
    "                        lat_flattened >= lat_min,\n",
    "                        lon_flattened <= lon_max,\n",
    "                        lat_flattened <= lat_max)))[0]\n",
    "\n",
    "    lon_flattened = lon_flattened[the_box]\n",
    "    lat_flattened = lat_flattened[the_box]\n",
    "    height_flattened = height_flattened[the_box]\n",
    "\n",
    "    u_flattened = grb_u.data[:, :, :].flatten()\n",
    "    u_flattened = u_flattened[the_box]\n",
    "    u_interp = NearestNDInterpolator(\n",
    "        (height_flattened, lat_flattened, lon_flattened),\n",
    "        u_flattened, rescale=True)\n",
    "    u_new = u_interp(radar_grid_alt, radar_grid_lat, radar_grid_lon)\n",
    "\n",
    "    v_flattened = grb_v.data[:, :, :].flatten()\n",
    "    v_flattened = v_flattened[the_box]\n",
    "    v_interp = NearestNDInterpolator(\n",
    "        (height_flattened, lat_flattened, lon_flattened),\n",
    "        v_flattened, rescale=True)\n",
    "    v_new = v_interp(radar_grid_alt, radar_grid_lat, radar_grid_lon)\n",
    "\n",
    "    w_flattened = grb_v.data[:, :, :].flatten()\n",
    "    w_flattened = w_flattened[the_box]\n",
    "    w_interp = NearestNDInterpolator(\n",
    "        (height_flattened, lat_flattened, lon_flattened),\n",
    "        w_flattened, rescale=True)\n",
    "    w_new = w_interp(radar_grid_alt, radar_grid_lat, radar_grid_lon)\n",
    "\n",
    "    del grb_u, grb_v, grb_w, lat, lon\n",
    "    del the_grib\n",
    "    gc.collect()\n",
    "\n",
    "    return u_new, v_new, w_new\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
